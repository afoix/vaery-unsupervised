# General paths for the project
paths:
  head_path: "/mnt/efs/aimbl_2025/student_data/S-GL/"
  metadata_filename: "2025-09-03_lDE20_Final_balanced_30tr-per-gene.pkl"
  data_path: "Ecoli_lDE20_Exps-0-1/"
  logs_headpath: "/mnt/efs/aimbl_2025/student_data/S-GL/tb_logs/marlin_contrastive/"
  checkpoint_path: "/mnt/efs/aimbl_2025/student_data/S-GL/tb_logs/marlin_contrastive/version_21/checkpoints/last.ckpt"
  embeddings_directory: '/mnt/efs/aimbl_2025/student_data/S-GL/embeddings/2025-09-03_embeddings_resnet_v1/'

# Model hyperparameters for SmallObjectResNet10Encoder
encoder:
  in_channels: 1
  widths: [32, 64, 128, 256]
  feature_stage: "layer4"
  layer4_dilate: 3             # captures long rods without further downsampling
  norm: "batch"
  # stride: [2, 1]
  gn_groups: 8
  drop_path_rate: 0.05
  mlp_hidden_dims: 512
  projection_dim: 128

# New key to specify the encoder module and class
encoder_module:
  path: "vaery_unsupervised.networks.marlin_resnet"
  class: "SmallObjectResNet10Encoder"

# Key to specify the model module and class
model_module:
  path: "vaery_unsupervised.networks.marlin_contrastive"
  class: "ContrastiveModule"

# Hyperparameters for the ContrastiveModule and training
contrastive_module:
  loss:
    temperature: 0.07
  lr: 0.001

# DataModule configuration
data_module:
  path: "vaery_unsupervised.dataloaders.marlin_dataloader.marlin_dataloader_hstack"
  class: "MarlinDataModule"
  split_ratio: 0.8
  batch_size: 512 #4096 #2048 # For training: 512  # 256 + 128
  num_workers: 8
  prefetch_factor: 2
  # prediction_mode: "batch" # NEW: 'single' or 'batch'

# MONAI transforms for data augmentation
transforms:
  - name: "RandFlipd"
    keys: ["positive"]
    spatial_axis: 0
    prob: 0.5
  - name: "RandFlipd"
    keys: ["positive"]
    spatial_axis: 1
    prob: 0.5
  - name: "RandGaussianNoised"
    keys: ["positive"]
    prob: 0.5
    mean: 0
    std: 0.5
    sample_std: True
  - name: "RandGaussianSmoothd"
    keys: ["positive"]
    sigma_x: [0.5, 1.5]
    sigma_y: [0.5, 1.5]
    prob: 0.5

# PyTorch Lightning Trainer settings
trainer:
  devices: "auto"
  accelerator: "gpu"
  strategy: "auto"
  precision: "16-mixed"
  max_epochs: 1000
  fast_dev_run: False
  log_every_n_steps: 100