    # Extract both embeddings and projections
#%%
import lightning as L
from typing import Callable
import numpy as np
import pytorch_lightning as pl
import torch
from iohub import open_ome_zarr
from iohub.ngff import Position
from monai.data import set_track_meta
from monai.transforms import Compose, ToTensord
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from monai.transforms import Compose, RandSpatialCrop, RandRotate, RandWeightedCrop, CenterSpatialCrop
import logging
from typing import Literal
import torch.nn.functional as F
from lightning import LightningModule
from monai.networks.nets.resnet import ResNetFeatures
from pytorch_metric_learning.losses import NTXentLoss, SelfSupervisedLoss
from torch import Tensor, nn
from typing_extensions import TypedDict

from vaery_unsupervised.dataloaders.hcs_dataloader_ryan import HCSDataModule
from vaery_unsupervised.networks.hcs_contrastive import ResNetEncoder, ContrastiveModule
from lightning.pytorch.loggers import TensorBoardLogger
from lightning.pytorch.callbacks import ModelCheckpoint
from pathlib import Path
import torchview
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
#%%
#Load data module
ome_zarr_path = "/mnt/efs/aimbl_2025/student_data/S-RM/full_dataset/RM_project_ome2.zarr"

data_module = HCSDataModule(
        ome_zarr_path=ome_zarr_path,
        source_channel_names=['mito','er','nuclei'],
        weight_channel_name='nuclei',
        crop_size=(128, 128),
        crops_per_position=4,
        batch_size=64,
        num_workers=6,
        split_ratio=0.8,
        normalization_transform=[],
        augmentations=[]
    )

data_module.setup(stage='val')
#%%

val_loader = data_module.val_dataloader()

#%%

ckpt_path = "/mnt/efs/aimbl_2025/student_data/S-RM/logs/contrastive_first/version_35/checkpoints/last.ckpt"

hcs_encoder_config = {
    "backbone": "resnet18",
    "in_channels": 3,
    "spatial_dims": 2,
    "embedding_dim": 512,
    "mlp_hidden_dims": 256,#256 originally 768
    "projection_dim": 64,#64 originally 128
    "pretrained": False,
}
hcs_encoder = ResNetEncoder(**hcs_encoder_config)

model = ContrastiveModule.load_from_checkpoint(
    ckpt_path,
    encoder=hcs_encoder,
    loss=SelfSupervisedLoss(NTXentLoss(temperature=0.07)),
    lr=1e-4,
)
model = model.cuda()
model.eval()

#%% 
#for plotting projections
# all_emb, all_proj, all_pos_info = [], [], []

# with torch.no_grad():
#     for batch in val_loader:
#         emb, proj = model.encoder(batch["anchor"].cuda())
#         all_emb.append(emb.cpu())
#         all_proj.append(proj.cpu())
#         all_pos_info.extend(batch["pos_info"])  # <-- collect position info here

# # concatenate tensors into numpy arrays
# all_emb = torch.cat(all_emb, dim=0).numpy()
# all_proj = torch.cat(all_proj, dim=0).numpy()
# #%%
# import pandas as pd

# # Make DataFrame from all_proj
# proj_df = pd.DataFrame(all_proj, columns=[f"proj_{i}" for i in range(all_proj.shape[1])])

# # Make DataFrame for pos info
# pos_df = pd.DataFrame(all_pos_info, columns=["pos_info"])
# pos_df[["row", "col", "field"]] = pos_df["pos_info"].str.split("/", expand=True)

# # Combine projections and position info into one table
# full_df = pd.concat([proj_df, pos_df], axis=1)

# # Optional: convert row/col/field to integers
# full_df["row"] = full_df["row"].astype(int)
# full_df["col"] = full_df["col"].astype(int)
# full_df["field"] = full_df["field"].astype(int)

# annotations_path= '/mnt/efs/aimbl_2025/student_data/S-RM'

# # Load annotation CSV
# annotations = pd.read_csv(f"{annotations_path}/annotations.csv")
# annotations = annotations.rename(columns={'row_num': 'row', 'column': 'col'})

# # Merge 'gene' into full_df
# full_df = full_df.merge(annotations[['gene', 'col','row']], on=['row', 'col'], how='left')

# # Check
# full_df.info()

#%%
#for plotting embeddings
all_emb, all_proj, all_pos_info = [], [], []

with torch.no_grad():
    for batch in val_loader:
        emb, proj = model.encoder(batch["anchor"].cuda())
        all_emb.append(emb.cpu())
        all_proj.append(proj.cpu())
        all_pos_info.extend(batch["pos_info"])  # <-- collect position info here

# concatenate tensors into numpy arrays
all_emb = torch.cat(all_emb, dim=0).numpy()
all_proj = torch.cat(all_proj, dim=0).numpy()
#%%
import pandas as pd

# Make DataFrame from all_proj
emb_df = pd.DataFrame(all_emb, columns=[f"proj_{i}" for i in range(all_emb.shape[1])])

# Make DataFrame for pos info
pos_df = pd.DataFrame(all_pos_info, columns=["pos_info"])
pos_df[["row", "col", "field"]] = pos_df["pos_info"].str.split("/", expand=True)

# Combine embeddings and position info into one table
full_df = pd.concat([emb_df, pos_df], axis=1)

# Optional: convert row/col/field to integers
full_df["row"] = full_df["row"].astype(int)
full_df["col"] = full_df["col"].astype(int)
full_df["field"] = full_df["field"].astype(int)

annotations_path= '/mnt/efs/aimbl_2025/student_data/S-RM'

# Load annotation CSV
annotations = pd.read_csv(f"{annotations_path}/annotations.csv")
annotations = annotations.rename(columns={'row_num': 'row', 'column': 'col'})

# Merge 'gene' into full_df
full_df = full_df.merge(annotations[['gene', 'col','row']], on=['row', 'col'], how='left')

# Check
full_df.info()

#%%
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce to 2D
pca = PCA(n_components=2)
proj_2d = pca.fit_transform(all_proj)

# Use the gene column for coloring
genes = full_df["gene"].astype(str)   # make sure it's string for grouping
unique_genes = genes.unique()
gene_to_idx = {g: i for i, g in enumerate(unique_genes)}
colors = [gene_to_idx[g] for g in genes]

# Plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(proj_2d[:, 0], proj_2d[:, 1], 
                      c=colors, cmap="tab20", alpha=0.6)

# # Legend with gene labels
# handles, _ = scatter.legend_elements(num=len(unique_genes))
# plt.legend(handles, unique_genes, title="Gene", bbox_to_anchor=(1.05, 1), loc="upper left")

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of Contrastive Embeddings by Gene")
plt.tight_layout()
plt.show()


#%%
import umap.umap_ as umap
import matplotlib.pyplot as plt

# Run UMAP on projections
umap_model = umap.UMAP(n_components=2, random_state=57)
proj_2d_umap = umap_model.fit_transform(all_proj)

# Get gene labels from full_df (must match index of all_proj)
gene_labels = full_df["gene"].fillna("NA")  # fallback if missing

# Map each gene to an integer color
unique_genes = sorted(gene_labels.unique())
gene_to_idx = {g: i for i, g in enumerate(unique_genes)}
colors = gene_labels.map(gene_to_idx)

# Plot UMAP scatter
plt.figure(figsize=(12, 10))
scatter = plt.scatter(proj_2d_umap[:, 0], proj_2d_umap[:, 1],
                      c=colors, cmap="tab20", alpha=0.6)

# # Legend with gene names
# handles, _ = scatter.legend_elements(num=len(unique_genes))
# plt.legend(handles, unique_genes, title="Gene",
#            bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=8)

plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.title("UMAP of Contrastive Embeddings by Gene")
plt.tight_layout()
plt.show()

#%%
#umap with only negcons
import umap.umap_ as umap
import matplotlib.pyplot as plt

# Run UMAP on projections
umap_model = umap.UMAP(n_components=2, random_state=57)
proj_2d_umap = umap_model.fit_transform(all_proj)

# Gene labels
gene_labels = full_df["gene"].fillna("NA")

# Build color array: grey by default, special color for negcon
colors = ["lightgrey"] * len(gene_labels)
for i, g in enumerate(gene_labels):
    if g.lower() == "negcon":
        colors[i] = "red"   # highlight negcon in red (change if you like)

# Plot
plt.figure(figsize=(12, 10))
plt.scatter(proj_2d_umap[:, 0], proj_2d_umap[:, 1],
            c=colors, alpha=0.6)

plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.title("UMAP of Contrastive Embeddings\nHighlighting negcon")
plt.tight_layout()
plt.show()
 #%%

 #pca of negcons only
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Run PCA
pca = PCA(n_components=2)
proj_2d = pca.fit_transform(all_proj)

# Gene labels
gene_labels = full_df["gene"].fillna("NA")

# Build colors: grey by default, highlight negcon
colors = ["lightgrey"] * len(gene_labels)
for i, g in enumerate(gene_labels):
    if g.lower() == "negcon":
        colors[i] = "red"   # negcon points in red

# Plot
plt.figure(figsize=(10, 8))
plt.scatter(proj_2d[:, 0], proj_2d[:, 1],
            c=colors, alpha=0.6)

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of Contrastive Embeddings\nHighlighting negcon")
plt.tight_layout()
plt.show()
#%%
##by row
import umap.umap_ as umap
import matplotlib.pyplot as plt

# Run UMAP
umap_model = umap.UMAP(n_components=2, random_state=57)
proj_2d_umap = umap_model.fit_transform(all_proj)

# Use the row column for coloring
rows = full_df["row"]

plt.figure(figsize=(12, 10))
scatter = plt.scatter(proj_2d_umap[:, 0], proj_2d_umap[:, 1],
                      c=rows, cmap="tab20", alpha=0.7)

# Add colorbar to show row values
cbar = plt.colorbar(scatter)
cbar.set_label("Row")

plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.title("UMAP of Contrastive Embeddings colored by Row")
plt.tight_layout()
plt.show()

#%%
#%%
#%%
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Run t-SNE on projections
tsne_model = TSNE(n_components=2, random_state=57, perplexity=30, max_iter=1000)
proj_2d_tsne = tsne_model.fit_transform(all_proj)

# Gene labels
gene_labels = full_df["gene"].fillna("NA")

# Build color array: grey by default, special color for negcon
colors = ["lightgrey"] * len(gene_labels)
for i, g in enumerate(gene_labels):
    if g.lower() == "negcon":
        colors[i] = "red"   # highlight negcon in red

# Plot
plt.figure(figsize=(12, 10))
plt.scatter(proj_2d_tsne[:, 0], proj_2d_tsne[:, 1],
            c=colors, alpha=0.6)

plt.xlabel("t-SNE-1")
plt.ylabel("t-SNE-2")
plt.title("t-SNE of Contrastive Embeddings\nHighlighting negcon")
plt.tight_layout()
plt.show()


#%%
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Reduce to 2D
pca = PCA(n_components=2)
proj_2d = pca.fit_transform(all_proj)

# Choose which genes to show
genes_to_show = ["negcon", "CASP3", "AURKB", "PLK1", "KRAS", "HSP90AB1"]


# Subset the data
mask = full_df["gene"].str.lower().isin([g.lower() for g in genes_to_show])
proj_subset = proj_2d[mask]
genes_subset = full_df.loc[mask, "gene"].str.upper()

# Assign colors dynamically
unique_genes = sorted(genes_subset.unique())
cmap = plt.get_cmap("tab10")   # tab10 has 10 distinct colors
gene_to_color = {g: cmap(i) for i, g in enumerate(unique_genes)}
colors = [gene_to_color[g] for g in genes_subset]

# Plot
plt.figure(figsize=(10, 8))
plt.scatter(proj_subset[:, 0], proj_subset[:, 1],
            c=colors, alpha=0.7)

# Legend
for g, c in gene_to_color.items():
    plt.scatter([], [], c=c, label=g)

plt.legend(title="Gene", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("PCA of Contrastive Embeddings\n(Selected Genes Only)")
plt.tight_layout()
plt.show()



#%%



















