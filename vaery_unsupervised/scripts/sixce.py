#test2

import os
# import sys
# import argparse
import numpy as np
import pandas as pd
# from torch.utils.data import DataLoader
import torch
import neptune
import zarr
import sys

from vaery_unsupervised.dataloaders.dataloader_sixce import SixceDataModule
from vaery_unsupervised.networks.contrastive_sixce import ResNetEncoder, ContrastiveModule
import pytorch_lightning as pl
from pytorch_metric_learning.losses import NTXentLoss, SelfSupervisedLoss
from lightning.pytorch.loggers import NeptuneLogger
# from lightning.pytorch.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks import ModelCheckpoint
from pathlib import Path
import warnings

import umap
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Optional, Union, Tuple


# Set mode
RUN_INFERENCE = True
checkpoint_to_load = "SIXCE-3/last.ckpt" # str: run_id/checkpoint_file_name.ckpt"

# Neptune experiment description
EXPERIMENT_DESCRIPTION = (f"SIXCE: Pilot")
NEPTUNE_TOKEN_PATH = "/dgx1nas1/storage/data/kamal/neptune_token"
with open(NEPTUNE_TOKEN_PATH, "r") as f:
    api_token = f.read().strip()


# Paths ----------------------------------------------------------------------------------------------------------------
DATA_PATH = '/dgx1nas1/storage/data/kamal/sixce_data'
DATAFRAMES_PATH = os.path.join(DATA_PATH, 'dataframes')
SAVED_DATA_PATH = os.path.join(DATA_PATH, "saved_datasets")
OME_ZARR_PATH = '/raid/data/temp_kamal/merged_mosaics.ome.zarr'
CHECKPOINT_PATH = os.path.join(DATA_PATH, "checkpoints")
SAVED_INFERENCE_PATH = os.path.join(DATA_PATH, "inferences")

# Training dataset parameters (can be ignored if loading existing dataset) ---------------------------------------------
RANDOM_SUBSET = 2000 # if -1, uses all samples. Note: existing pt dataset with 0_001k cells only contains 1 gene
# Unsorted list of stains to include in the training. Will get sorted downstream based on the OME-Zarr attributes
STAINS_unsorted = ['DAPI', 'Cellbound1', 'Cellbound2', 'Cellbound3', 'Ki67', 'WT1', 'PolyT'] # 'mask' autogenerated anyway
N_MASKS = 0 # cell mask (numeric bool)
GENE_COUNT_THRESHOLD = -1
APPLY_DATA_AUGMENTATION = True
TENSOR_TYPE = torch.float32
INPUT_DIM = 200 #206  # crop size
LAZY_LOAD = True
SPOT_REPRESENTATION_TYPE = 'psf' # point spread function 'psf' or 'binary'
SPOT_PSF_GAUSSIAN_SIGMA = 1.0 # sigma of gaussian patch applied in psf representation
SPOT_PSF_GAUSSIAN_KERNEL_SIZE = 9 # size of gaussian kernel in psf representation (side of square or diameter of circle)
SPOT_PSF_PX_VAL_NORMALIZATION_METHOD = 'peak' # gaussian patch pixel val norm: peak, sum or sum_corrected
PEAK_NORMALIZATION_PEAK_VALUE = 0.8 # Value of peak if using the peak normalization method for PSF
CIRCULAR_PSF = True # If false, PSFs are square-truncated
PADDING = 4 # spot padding for binary spot representation

# Training parameters --------------------------------------------------------------------------------------------------
GLOBAL_SEED = 137
LEARNING_RATE = 0.0005
EPOCHS = 100
BATCH_SIZE = 50
N_WORKERS = 10
LOSS_TEMPERATURE=0.1
VALIDATION_FRACTION=0.2
# Loss function parameters ---------------------------------------------------------------------------------------------

# Experiment variables -------------------------------------------------------------------------------------------------
DRY_RUN = False
EPOCHS = 1 if DRY_RUN else EPOCHS


# Constructing file name string based on RANDOM_SUBSET
if RANDOM_SUBSET == -1:
    subset_size_str = "all"
elif RANDOM_SUBSET >= 1000:
    subset_size_str = f"{RANDOM_SUBSET // 1000}k"
else:
    subset_size_str = f"{RANDOM_SUBSET / 1000:.5f}".lstrip('0').rstrip('0').rstrip('.').replace('.', '0_') + 'k'

expected_filename = f"sixce_dataset_{subset_size_str}Cells.pt"
saved_dataset_path = os.path.join(
    SAVED_DATA_PATH,
    expected_filename
)

# Preparing data
# Load existing dataset if possible
if os.path.exists(saved_dataset_path):
    print("Loading existing Dataset...")
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=FutureWarning)
        dataset = torch.load(saved_dataset_path)

    # Extracting stored variables
    N_GENES = dataset.n_genes
    INPUT_DIM = dataset.input_dim
    STAINS = dataset.selected_stains
    N_STAINS = len(STAINS)
    # N_MASKS = len(train_dataset.stain_order) - N_STAINS
    N_MASKS = dataset.n_masks  # mkw
    STAIN_TO_INDEX = {s: i for i, s in enumerate(dataset.stain_order) if s != "mask"}
    APPLY_DATA_AUGMENTATION = dataset.apply_data_augmentation
    TENSOR_TYPE = dataset.tensor_type
    REPRESENTATION_TYPE = dataset.representation_type
    SPOT_PSF_GAUSSIAN_SIGMA = dataset.gaussian_patch_sigma
    SPOT_PSF_GAUSSIAN_KERNEL_SIZE = dataset.gaussian_kernel_size
    PSF_NORMALIZATION = dataset.psf_normalization
    RANDOM_SUBSET = dataset.random_subset

    # Initializing None vars
    zarr_attributes=None
    transcript_df=None
    cell_metadata_df=None
    cell_boundaries_df=None

else:
    dataset=None
    # Loading OME-Zarr metadata
    print("Loading OME-Zarr metadata")
    zarr_attributes = zarr.open(f"{OME_ZARR_PATH}/0", mode="r").attrs

    # Sorting the STAINS_unsorted list and indexing as per OME-Zarr, same as the Dataset class
    ome_channels = [ch["label"] for ch in zarr_attributes["omero"]["channels"]]
    STAINS = [label for label in ome_channels
              if label in STAINS_unsorted]
    STAIN_TO_INDEX = {stain: i for i, stain in enumerate(STAINS)}
    # Guard against typos
    missing = set(STAINS_unsorted) - set(STAINS)
    if missing:
        raise ValueError(f"These stains are not in the OME-Zarr: {missing}")
    N_STAINS = len(STAINS)

    print("Loading detected_transcripts.csv")
    # Load dataframe with the detected transcripts
    transcript_df_path = os.path.join(DATAFRAMES_PATH, "detected_transcripts.csv")
    transcript_df = pd.read_csv(transcript_df_path)

    # Removing blank probes/rounds
    blank_rows = transcript_df["gene"].str.startswith("Blank-")
    transcript_df.drop(index=transcript_df.index[blank_rows], inplace=True)

    # Apply gene count thresholding
    if GENE_COUNT_THRESHOLD != -1:
        print("Applying gene filtering")
        # Keep only barcode_ids that occur more than the threshold
        barcode_counts = transcript_df['barcode_id'].value_counts()
        valid_barcodes = barcode_counts[barcode_counts > GENE_COUNT_THRESHOLD].index
        transcript_df = transcript_df[transcript_df['barcode_id'].isin(valid_barcodes)]

    # Re-indexing (barcode_id) in-place based on gene name order (strings)
    uniq_genes = np.sort(transcript_df["gene"].unique())
    gene_to_id = {g: i for i, g in enumerate(uniq_genes)}

    N_GENES = len(uniq_genes)
    if N_GENES <= 255:
        id_dtype = np.uint8
    else:  # In case of a bigger dataset
        id_dtype = np.uint16
    transcript_df["barcode_id"] = transcript_df["gene"].map(gene_to_id).astype(id_dtype)

    print("Loading cell_metadata.csv")
    # Load cell metadata df
    cell_metadata_df_path = os.path.join(DATAFRAMES_PATH, "cell_metadata.csv")
    cell_metadata_df = pd.read_csv(cell_metadata_df_path)

    print("Loading cell_boundaries.parquet")
    # Load cell boundaries df
    cell_boundaries_df_path = os.path.join(DATAFRAMES_PATH, "cell_boundaries.parquet")
    cell_boundaries_df = pd.read_parquet(cell_boundaries_df_path)

datamodule_config = {
            "data_path": OME_ZARR_PATH,
            "dataset_export_path": saved_dataset_path,
            "batch_size": BATCH_SIZE,
            "prefetch_factor": 2,
            "pin_memory": False,
            "persistent_workers": False,
            "n_workers": N_WORKERS,
            "val_fraction": VALIDATION_FRACTION,
            "zarr_attributes": zarr_attributes,
            "transcripts_df": transcript_df,
            "cell_metadata_df": cell_metadata_df,
            "cell_boundary_df": cell_boundaries_df,
            "input_dim": INPUT_DIM,
            "n_genes": N_GENES,
            "existing_dataset": dataset,
            "selected_stains": STAINS,
            "n_masks": N_MASKS,
            "padding": PADDING,
            "apply_data_augmentation": APPLY_DATA_AUGMENTATION,
            "tensor_type": TENSOR_TYPE,
            "include_stains": LAZY_LOAD,
            "representation_type": SPOT_REPRESENTATION_TYPE,
            "gaussian_patch_sigma": SPOT_PSF_GAUSSIAN_SIGMA,
            "gaussian_kernel_size": SPOT_PSF_GAUSSIAN_KERNEL_SIZE,
            "psf_normalization": SPOT_PSF_PX_VAL_NORMALIZATION_METHOD,
            "random_subset": RANDOM_SUBSET,
            "peak_norm_peak_val": PEAK_NORMALIZATION_PEAK_VALUE,
            "circular_psf": CIRCULAR_PSF,
            "global_seed": GLOBAL_SEED
}

logged_dataset_config = {
            **datamodule_config,
            "gene_threshold": GENE_COUNT_THRESHOLD,
            "tensor_type": str(TENSOR_TYPE),
            "stains": ",".join(STAINS)
        }

def main():
    pl.seed_everything(GLOBAL_SEED, workers=True)

    dm = SixceDataModule(**datamodule_config)
    encoder = ResNetEncoder(
        backbone="resnet18",
        in_channels=N_GENES + N_STAINS,
        spatial_dims=2,
        embedding_dim=512,
        mlp_hidden_dims=768,
        projection_dim=128
    )

    if not RUN_INFERENCE:
        model = ContrastiveModule(encoder=encoder, lr=LEARNING_RATE, optimizer=None, temperature=LOSS_TEMPERATURE)

        # Initialize Neptune
        logger = NeptuneLogger(
            api_key=api_token,
            project="BroadImagingPlatform/SIXCE",
            tags=["training", "resnet18"],
            log_model_checkpoints=False,
        )

        neptune_run = logger.experiment
        neptune_run["Experiment"] = EXPERIMENT_DESCRIPTION
        run_id = neptune_run["sys/id"].fetch()
        neptune_run["dataset_config"] = logged_dataset_config

        # Local checkpoints
        current_checkpoint_path = os.path.join(CHECKPOINT_PATH, run_id)
        os.mkdir(current_checkpoint_path)

        trainer = pl.Trainer(
            accelerator="gpu",
            num_nodes=1,
            precision="16-mixed",
            strategy="auto",
            max_epochs=EPOCHS,
            logger=logger,
            devices=1,
            # fast_dev_run=True, #Only when debugging
            callbacks = [ModelCheckpoint( dirpath=current_checkpoint_path,
                filename="{epoch}-{step}", save_last=True, monitor="loss/val", save_top_k=8, every_n_epochs=1)],
        )

        # Train
        dm.setup(stage="fit")
        trainer.fit(model, datamodule=dm)

    elif checkpoint_to_load is not None:
        model_path = os.path.join(CHECKPOINT_PATH, checkpoint_to_load)
        model = ContrastiveModule.load_from_checkpoint(model_path, encoder=encoder)
        model_name = checkpoint_to_load.split('/')[0]

        dm.setup(stage="predict")

        # Create trainer for inference
        trainer = pl.Trainer(
            accelerator="gpu",
            num_nodes=1,
            precision="16-mixed",
            strategy="auto",
            max_epochs=EPOCHS,
            logger=False,
            devices=1,
        )

        # Run prediction
        print("Running inference...")
        predictions = trainer.predict(model, datamodule=dm)

        # Extract both embeddings and projections
        embeddings = torch.cat([pred['embeddings'] for pred in predictions], dim=0).numpy()
        projections = torch.cat([pred['projections'] for pred in predictions], dim=0).numpy()

        # Save inference
        np.save(f"{SAVED_INFERENCE_PATH}/{model_name}_embeddings.npy", embeddings)
        np.save(f"{SAVED_INFERENCE_PATH}/{model_name}_projections.npy", projections)




if __name__ == "__main__":
    main()

